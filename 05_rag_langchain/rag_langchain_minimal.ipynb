{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c323bf",
   "metadata": {},
   "source": [
    "# RAG – LangChain (EN/VN)\n",
    "**Objective/Mục tiêu**: Minimal RAG over local docs; embedding → vector store → retriever → LLM. Replace LLM with local or API-backed model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940c2cc",
   "metadata": {},
   "source": [
    "\n",
    "# !pip install -q langchain langchain-community langchain-text-splitters faiss-cpu sentence-transformers transformers pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, glob\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Prepare corpus folder\n",
    "os.makedirs(\"docs\", exist_ok=True)\n",
    "with open(\"docs/sample.txt\", \"w\") as f:\n",
    "    f.write(\"Retrieval-augmented generation (RAG) combines retrieval with generation to reduce hallucination.\")\n",
    "\n",
    "# Load files\n",
    "docs = []\n",
    "for path in glob.glob(\"docs/*.txt\"):\n",
    "    docs.extend(TextLoader(path).load())\n",
    "\n",
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=60)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embeddings + Vector store\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vs = FAISS.from_documents(chunks, emb)\n",
    "\n",
    "# Retriever\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Simple local LLM via transformers pipeline (replace with API/vLLM later)\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "gen = pipeline(\"text-generation\", model=mdl, tokenizer=tok, max_new_tokens=256)\n",
    "\n",
    "def answer(query):\n",
    "    rel_docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\n",
    "\n",
    "\".join([d.page_content for d in rel_docs])\n",
    "    prompt = f\"Use the CONTEXT to answer. Be concise.\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "ANSWER:\"\n",
    "    out = gen(prompt)[0][\"generated_text\"]\n",
    "    return out\n",
    "\n",
    "print(answer(\"What is RAG?\"))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
