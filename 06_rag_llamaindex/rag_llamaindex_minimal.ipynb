{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde5e1da",
   "metadata": {},
   "source": [
    "# RAG – LlamaIndex + Simple Tool (EN/VN)\n",
    "**Objective/Mục tiêu**: Build index from local docs and query; demonstrate a trivial tool (calculator).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f9c93",
   "metadata": {},
   "source": [
    "\n",
    "# !pip install -q llama-index-core llama-index-embeddings-huggingface llama-index-vector-stores-faiss sentence-transformers transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdec0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "os.makedirs(\"docs_li\", exist_ok=True)\n",
    "with open(\"docs_li/sample.txt\", \"w\") as f:\n",
    "    f.write(\"RAG augments LLMs by retrieving relevant text chunks and conditioning generation to improve faithfulness.\")\n",
    "\n",
    "reader = SimpleDirectoryReader(\"docs_li\")\n",
    "docs = reader.load_data()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vec_dim = embed_model._core.embedding_dim()\n",
    "faiss_index = faiss.IndexFlatIP(vec_dim)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(docs, storage_context=storage_context, embed_model=embed_model)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=4)\n",
    "resp = query_engine.query(\"Describe RAG in one line.\")\n",
    "print(resp)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
