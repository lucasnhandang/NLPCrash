{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88a90f2",
   "metadata": {},
   "source": [
    "# vLLM Client – OpenAI-Compatible (EN/VN)\n",
    "**Objective/Mục tiêu**: Sample client for a locally served vLLM endpoint. Server commands included as reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606a0e2",
   "metadata": {},
   "source": [
    "\n",
    "# Server reference (run separately on your machine with GPU):\n",
    "# vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-0.5B-Instruct --host 0.0.0.0 --port 8000 --max-model-len 4096\n",
    "# Then use the standard OpenAI Python SDK by pointing api_base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d65b5",
   "metadata": {},
   "source": [
    "\n",
    "# !pip install -q openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "api_base = os.environ.get(\"VLLM_API_BASE\", \"http://localhost:8000/v1\")\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\", \"EMPTY\")  # vLLM ignores key by default\n",
    "\n",
    "client = OpenAI(base_url=api_base, api_key=api_key)\n",
    "\n",
    "chat = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain RAG in one sentence.\"}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    ")\n",
    "print(chat.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
