{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93804d08",
   "metadata": {},
   "source": [
    "# Named Entity Recognition – RoBERTa (EN/VN)\n",
    "**Objective/Mục tiêu**: Token classification on CoNLL-2003 with BIO labels.\n",
    "**Inputs**: `datasets`, `transformers`, `evaluate`.\n",
    "**Outputs**: F1, model checkpoint, label alignment demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98c8cc",
   "metadata": {},
   "source": [
    "\n",
    "# !pip install -q transformers datasets evaluate seqeval accelerate torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            new_labels.append(labels[word_idx])\n",
    "        else:\n",
    "            # Inside subword: convert B- to I- when applicable\n",
    "            label = labels[word_idx]\n",
    "            if label % 2 == 1: # B-*\n",
    "                label += 1     # I-*\n",
    "            new_labels.append(label)\n",
    "        previous_word_idx = word_idx\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align(batch):\n",
    "    tokenized = tokenizer(batch[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    aligned = []\n",
    "    for i, labels in enumerate(batch[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(i)\n",
    "        aligned.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized[\"labels\"] = aligned\n",
    "    return tokenized\n",
    "\n",
    "tokenized = dataset.map(tokenize_and_align, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=2)\n",
    "    true_labels = p.label_ids\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, true_labels)\n",
    "    ]\n",
    "    true_labels_seq = [\n",
    "        [label_list[l] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, true_labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels_seq)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"]}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"].select(range(5000)),\n",
    "    eval_dataset=tokenized[\"validation\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a38b38",
   "metadata": {},
   "source": [
    "\n",
    "# Save artifacts\n",
    "trainer.save_model(\"./model_roberta_conll2003\")\n",
    "tokenizer.save_pretrained(\"./model_roberta_conll2003\")\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Saved model + metrics.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
